import json
import time
from tqdm import tqdm
from openai import OpenAI
import os

# ==============================================================================
# C O N F I G U R A T I O N   S E C T I O N
# ==============================================================================
# --- Configure your entire experiment here ---

CONFIG = {
    "analysis": {
        "enabled": True,  

        "input_filename": "",  # Input file path

        "api_keys": [  # List of API keys

        ],

        "api_base_url": "https://XXXXXXX/v1",

        "analysis_model": "deepseek-ai/DeepSeek-V3.1",  # Model used for analysis

        "output_filename": "",  # Output file path

        "max_concurrent_requests": 10,  # Maximum concurrent API requests
        "batch_size": 5,  # Number of problems processed per batch
        "request_timeout": 30,  # Timeout for each API request in seconds
        "rate_limit_delay": 0.5,  # Delay between API requests to avoid rate limits
    }
}


# ===================================================================
# ===================================================================
class APIKeyManager:
    def __init__(self, keys):
        if not keys:
            raise ValueError("API key list cannot be empty")
        self.keys = keys
        self.current_index = 0

    def get_key(self):
        key = self.keys[self.current_index]
        return key

    def rotate_key(self):
        self.current_index = (self.current_index + 1) % len(self.keys)

# ===================================================================
# Analysis function
# ===================================================================
def analyze_reasoning(text_to_analyze: str, client: OpenAI, model: str) -> dict:
    if not text_to_analyze:
        return {"success": True, "analysis": []}

    full_analysis_prompt = f"""
Below is a chain-of-reasoning generated by a Language Model when attempting to solve a math problem. Evaluate this chain-of-reasoning to determine whether it demonstrates beneficial problem-solving behaviors that deviate from typical linear, monotonic reasoning patterns commonly observed in language models.
<start_of_reasoning>
{text_to_analyze}
</end_of_reasoning>
Specifically, actively identify and emphasize beneficial behaviors such as:
(1) Backtracking: Explicitly revising approaches upon identifying errors or dead ends
(e.g., "This approach won't work because...").
(2) Verification: Systematically checking intermediate results or reasoning steps
(e.g., "Let's verify this result by...").
(3) Subgoal Setting: Breaking down complex problems into smaller, manageable steps
(e.g., "To solve this, we first need to...").
(4) Enumeration: Solving problems by exhaustively considering multiple cases or
possibilities.
Additionally, remain attentive to and encourage the identification of other
beneficial behaviors not explicitly listed here, such as creative analogies,
abstraction to simpler cases, or insightful generalizations.
Important:
Your response MUST be a single, valid JSON object. This object must contain one root key, "analysis", which holds a list of objects.
Each object in the list represents one beneficial behavior you identify.
Each object must contain three string keys: "behaviour", "example", and "justification".
"behaviour": Must be one of the exact behavior names listed above (e.g., "Backtracking", "Verification").
"example": Must be a direct, contiguous quote from the reasoning chain that serves as evidence.
"justification": A brief, one-sentence explanation of why the provided example demonstrates the specified behaviour.
If no beneficial behaviors are observed, the "analysis" list MUST be an empty list [].
Provide your evaluation clearly, formatted as follows:
{{
  "analysis": [
    {{

      "behaviour": "Subgoal Setting",
      "example": "To solve this, we first need to calculate the area of the circle, and then subtract the area of the square.",
      "justification": "The model explicitly breaks the problem down into two distinct calculation steps before execution."
    }},
    {{
      "behaviour": "Verification",
      "example": "Let me double check the calculation: 23 * 2 + 12 * 4 = 46 + 48 = 94. This matches the given number of feet.",
      "justification": "The model checks its final computed numbers against the constraints mentioned in the original problem."
    }}
  ]
}}"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": full_analysis_prompt}],
            temperature=0.2,
            response_format={"type": "json_object"},
            timeout=CONFIG["analysis"]["request_timeout"]
        )
        content = response.choices[0].message.content
        return {"success": True, "analysis": json.loads(content).get("analysis", [])}
    except json.JSONDecodeError:
        return {"success": False, "error": "API did not return valid JSON", "raw_response": content}
    except Exception as e:
        return {"success": False, "error": str(e)}

# ===================================================================
# Main analysis workflow
# ===================================================================
def run_analysis_stage_sync():
    cfg = CONFIG["analysis"]
    input_file = cfg["input_filename"]
    output_file = cfg["output_filename"]

    if not os.path.exists(input_file):
        print(f"Error: Input file '{input_file}' does not exist")
        return

    with open(input_file, 'r', encoding='utf-8') as f:
        all_data = json.load(f)

    key_manager = APIKeyManager(cfg["api_keys"])
    total_problems = len(all_data)
    total_success = 0
    total_failures = 0

    for problem_idx, problem in enumerate(tqdm(all_data, desc="Analyzing Problems")):
        for model_name, model_output in problem["model_outputs"].items():
            if "analysis_result" in model_output:
                continue

            text_to_analyze = model_output.get("thinking_process", "")
            max_retries = len(cfg["api_keys"])
            success = False

            for attempt in range(max_retries):
                api_key = key_manager.get_key()
                client = OpenAI(api_key=api_key, base_url=cfg["api_base_url"])
                result = analyze_reasoning(text_to_analyze, client, cfg["analysis_model"])

                # Output detailed debug information
                print(f"\n[DEBUG] Problem {problem_idx}, Model {model_name}, Attempt {attempt+1}/{max_retries}")
                print(f"[DEBUG] Using API Key index: {key_manager.current_index}")
                print(f"[DEBUG] Call success: {result.get('success')}")
                if result.get("raw_response"):
                    print(f"[DEBUG] Raw response: {result['raw_response'][:300]}...")  # Only show first 300 chars
                if not result.get("success"):
                    print(f"[DEBUG] Error: {result.get('error')}")

                if result.get("success"):
                    model_output["analysis_result"] = {
                        "success": True,
                        "analysis": result["analysis"],
                        "raw_response": result.get("raw_response")
                    }
                    success = True
                    total_success += 1
                    break
                else:
                    model_output["analysis_result"] = {
                        "success": False,
                        "error": result.get("error"),
                        "raw_response": result.get("raw_response")
                    }
                    key_manager.rotate_key()
                    time.sleep(cfg["rate_limit_delay"])

            if not success:
                print(f"‚ùå Problem index {problem_idx}, Model {model_name} failed with all API Keys")
                total_failures += 1

    # Save results
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)

    # Summary
    print("\n================ Debug Summary ================")
    print(f"Total problems: {total_problems}")
    print(f"Successfully analyzed: {total_success}")
    print(f"Failed analyses: {total_failures}")
    print(f"Analysis results saved to: '{output_file}'")
    print("=============================================\n")

# ===================================================================
# Execute
# ===================================================================
if __name__ == "__main__":
    if CONFIG["analysis"]["enabled"]:
        run_analysis_stage_sync()
